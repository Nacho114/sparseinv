%%% document layout
\documentclass[paper=A4, fontsize=11pt]{scrartcl}
\setlength\parindent{0pt}
\usepackage[margin=1in]{geometry}

%%% packages
\usepackage{amsmath,amssymb,amsthm}
\usepackage{bbm}
\usepackage{complexity}
\usepackage{tcolorbox}
\usepackage{mathrsfs}

\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\PassOptionsToPackage{numbers,sort,compress}{natbib}

\RequirePackage[pagebackref,colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{url}


%%% hacks
\setlength\parindent{0pt}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}
\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}

%%% macros
\let\oldpr\Pr
\renewcommand{\Pr}[1]{\oldpr\left( #1 \right)}
\newcommand{\Prsub}[2]{\oldpr_{#1}\left( #2 \right)}
\newcommand{\EX}[1]{{{\mathbb{E}}\left[#1\right]}}
\newcommand{\EXSUB}[2]{{{\mathbb{E}}_{#1}\left[#2\right]}}
\newcommand{\VAR}[1]{\text{Var}\left[#1\right]}
\newcommand{\COVAR}[2]{\text{Cov}\left[#1,\, #2\right]}
\newcommand{\ZO}{\{0, \, 1\}}
\newcommand{\median}[1]{\text{med}\left\lbrace #1\right\rbrace}
\newcommand{\order}[1]{\mathcal{O}\left(#1\right)}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\indicator}[1]{\mathbf{1}_{\left[#1\right]}}
\newcommand{\twosum}[2]{\sum_{\substack{#1\\#2}}}
\newcommand{\support}[1]{\textup{sup}\left(#1\right)}
\newcommand{\partspace}{\vspace{.3cm}}
\newcommand{\exspace}{\vspace{.8cm}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

%%% envs def
\newtheorem{lemma}{Lemma}[section]
%\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}[section]
%\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

%\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{bgd}{Background}

\newcommand{\dk}{g_{kk}} % D_kk
\newcommand{\dktil}{\tilde{g}_{kk}} % Dtil_kk


%%% title
\title{Sparse Approximate Inverse Preconditioner}
\subtitle{Computational Linear Algebra}
\author{Sidak Pal Singh}

\begin{document}
	\maketitle
	%\textbf{Disclaimer: } This project follows closely a project given in a different class covering convex optimization.
	\begin{abstract}
	
	
	The objective is to construct a sparse approximate preconditioner M for a given sparse matrix A. More specifically, we look for a right preconditioner M such that $\|I-A M\|_{F}$ is minimized. 
	We derive one of the most successful general-purpose methods for achieving this tasks is SPAI by Grote and Huckle \cite{grote} and provide a detailed proof of their key theorem. Further, we perform empirical experiments and reproduce the results in their paper. Also, we verify the performance of the preconditioners when used together with a linear system solver (like BICGSTAB). Lastly, we discuss another variant of sparse approximate inverse (MR) and illustrate it numerically on matrices arising in various applications.

\end{abstract}

	\newpage
	
%	\section{Prelude: A small lemma}
%	
%	Before we start our journey into the sparse inverse solvers, we introduce a small lemma:
%	
%	\begin{lemma}
%		
%		Let $A$ be a matrix and $r, e, m$ be vectors s.t. $r = e - Am$. If we want to minimize the norm of $\hat{r}$; where $\hat{r} = e - A\hat{m}$, s.t. $\hat{m}$ lies along the path $m + \alpha r$. \\
%		Then, the optimal step size is $\alpha = \frac{\langle\,r,Ar\rangle}{\|Ar\|_{2}^{2}}$
%		\\\\
%		Proof 
%		\\\\
%		We compute the gradient of $\|\hat{r}\|_{2}$ w.r.t. $\alpha$ to find the optimum step size. 
%		
%		
%		
%		
%		\begin{equation}
%		\begin{aligned}
%		\frac{\partial \|\hat{r} \|_{2}^{2}}{\partial \alpha} 
%		& = \frac{\partial \|e - A\hat{m}\|_{2}^{2}}{\partial \alpha} \\
%		& = \frac{\partial \|e - A(m + \alpha r)\|_{2}^{2}}{\partial \alpha} \\
%		& = 2\alpha \|Ar\|_{2}^{2}  + 2(Am)^{T}Ar - 2e^{T}Ar \\
%		& = 2\alpha \|Ar\|_{2}^{2}  + 2(Am - e)^{T}Ar \\
%		& = 2\alpha \|Ar\|_{2}^{2}  - 2r^{T}Ar \\
%		\\
%		\end{aligned}
%		\end{equation}
%		
%		By setting the derivative to $0$ we find that $\alpha = \frac{\langle\,r,Ar\rangle}{\|Ar\|_{2}^{2}}$
%		
%		
%		
%		
%	\end{lemma}
%	
%	\begin{lemma}
%		Let $A$ be a matrix and $r, e, m$ be vectors as before. 
%		
%		Then
%		
%		\begin{equation}
%		\min _{\mu_{j}}\left\|r+\mu_{j} A e_{j}\right\|_{2}
%		\end{equation}
%		
%		Has the following solution
%		
%		\begin{equation}
%		\mu_{j}=-\frac{r^{T} A e_{j}}{\left\|A e_{j}\right\|_{2}^{2}}
%		\end{equation}
%		
%		Proof
%		\\
%		We can use verbatim the proof of lemma 1.
%	\end{lemma}
%	
%	\newpage
	
\section{SParse Approximate Inverse (SPAI)}

\paragraph{Problem description.} The aim of SPAI \cite{grote} is to find a parallel preconditioner for the solution of linear system of equations. This is achieved by explicitly finding a sparse approximate inverse of $A$, which is later applied as a preconditioner for an iterative method like BICGSTAB or GMRES. The SPAI algorithm  does so by minimizing the Frobenius norm since this choice leads to inherent parallelism as the columns of  $M$ are independent of one another (see equation \eqref{eq:frob}) and is more tractable in comparison to minimizing the 1-norm or 2-norm.

\begin{equation}\label{eq:frob}
\|A M-I\|_{F}^{2}=\sum_{k=1}^{n}\left\|(A M-I) e_{k}\right\|_{2}^{2}, 
\end{equation}

where $n$ is the number of columns of $M$ and $e_k$ denotes the $k^{\text{th}}$ canonical basis vector. \\

Thus, it amounts to solving these $n$ independent least squares problems for each $k$


\begin{equation}\label{eq:indp}
\min _{m_{k}}\left\|A m_{k}-e_{k}\right\|_{2}, \quad k=1, \ldots, n
\end{equation}

We start with an initial sparsity pattern for $M$, like diagonal (for example $M = I$, where $I$ is the identity matrix). Then we seek to iteratively improve the columns, while trying to maintain the sparsity of $M$. 

\paragraph{Notation.}
\begin{itemize}
	\item Let \(\mathcal{J}\) be the set of indices \(j\) such that \(m_{k}(j) \neq 0\).
	\item The reduced vector of unknowns \(m_{k}(\mathcal{J})\) is denoted by \(\hat{m}_{k}\).
	\item Let \(\mathcal{I}\) be the set of
	indices \(i\) such that the row \(A(i, \mathcal{J})\) is not equal to zero.
	\item The resulting submatrix \(A(\mathcal{I}, \mathcal{J})\) is then denoted as \(\hat{A}\). Correspondinly, define \(\hat{e}_{k}=e_{k}(\mathcal{I}).\)
	\item Let \(n_{1}=|\mathcal{I}|\) and \(n_{2}=|\mathcal{J}|\).
	
\end{itemize}

Now, solving equation \eqref{eq:indp} is equivalent to the following problems of much smaller size as compared to original problem:

\begin{equation}
\min _{\hat{m}_{k}}\left\|\hat{A} \hat{m}_{k}-\hat{e}_{k}\right\|_{2}.
\end{equation}

We solve this reduced least squares problem by QR decomposition, i.e., 
\begin{equation}
\hat{A}=Q\left(\begin{array}{l}{R} \\ {0}\end{array}\right),
\end{equation}

where $Q \in \mathbb{R}^{n_{1} \times n_{1}}$ is an orthogonal matrix and \(R\) is a nonsingular upper triangular matrix  $\in \mathbb{R}^{n_{2} \times n_{2}}$. Let \(\hat{c}=Q^{T} \hat{e}_{k},\) then the solution of the least squares problem is,

\begin{equation}
\hat{m}_{k}=R^{-1} \hat{c}\left(1 : n_{2}\right).
\end{equation}


After solving this least square problem for every column, we get an approximate inverse. Next,we augment the sparsity structure to improve our current solution. \\


In particular, we add indices that result in the best reduction in error,  \(\left\|A m_{k}-e_{k}\right\|_{2}\). Let us denote the residual as,

\begin{equation}
r=A( ., \mathcal{J}) \hat{m}_{k}-e_{k}
\end{equation}


If $r = 0$, then we have exactly the $k_{th}$ column of $A^{-1}$. So  assume $r \neq 0$ and let $\mathcal{L}$ be the set of indices $\ell$ s.t. $r(\ell) \neq 0$. For every $ \ell \in \mathcal{L} $, consider \(\mathcal{N}_{\ell}\) indicating the nonzero elements of $A(\ell,.)$ which are not in $\mathcal{J}$ yet. Thus, we now have at our disposal a set of potential candidates ($\tilde{\mathcal{J}}$) to add to $\mathcal{J}$.


\begin{equation}
\tilde{\mathcal{J}}=\bigcup_{\ell \in \mathcal{L}} \mathcal{N}_{\ell}
\end{equation}

In order to find the best candidates, for every \(j \in \tilde{\mathcal{J}}\) we compute the following one dimensional minimization problem, that minimizes the residual based on adding index $j$.

\begin{equation}\label{eq:ls}
\min _{\mu_{j}}\left\|r+\mu_{j} A e_{j}\right\|_{2}
\end{equation}

%Essentially, $\mu_j$ plays the role of the potential entry of $\hat{m)_k$ which was zero before. 
The solution to \eqref{eq:ls} can be found by taking the derivative (of the square) and setting it to 0, 

\begin{equation}
\begin{aligned}
\frac{\partial \left\|r+\mu_{j} A e_{j}\right\|_{2}^2}{\partial \mu_{j}} 
& = 2r^{T} A e_{j} + 2\mu_{j}\|A e_{j}\|_{2}^{2}.
\end{aligned}
\end{equation}

Thus, we obtain: 

\begin{equation}\label{eq:muj}
\mu_{j}=-\frac{r^{T} A e_{j}}{\left\|A e_{j}\right\|_{2}^{2}}
\end{equation}

Hence, we compute the new residual for every such $j$ (i.e.  $\rho_j = r+\mu_{j} A e_{j}$), based on the optimal value found above in equation \eqref{eq:muj}. Further, 

\begin{equation}
\rho_{j}^{2}=\|r\|_{2}^{2}-\frac{\left(r^{T} A e_{j}\right)^{2}}{\left\|A e_{j}\right\|_{2}^{2}}.
\end{equation}

Now, there is at least one index $j \in \tilde{\mathcal{J}}$ s.t. \(r^{T} A e_{j} \neq 0\), which will lead to a smaller residual. Otherwise, we will have, 

\begin{equation}
0=r(\mathcal{L})^{T} A(\mathcal{L}, \tilde{\mathcal{J}})
\end{equation}


This would imply that \(r(\mathcal{L})\) is zero, as \(A(\mathcal{L}, .)\) has full rank, which would be a contradiction to our assumption that $r \neq 0$. Next, we choose $s$ most profitable indices (i.e., with smallest $\rho_j$) and finally repeat this procedure until convergence is attained ($\|r\|_{2} \leq \varepsilon$). 

\newpage
	
	
\section{The Grote and Huckle theorem}

\begin{bgd} Assume $M$ is the approximate inverse of $A$ obtained from SPAI algorithm, and let $m_k$ denote the $k^{\text{th}}$ column and $r_k$ be the corresponding residual. We assume that at convergence, 
\begin{equation}\label{eq:conv}
\| r_k\| = \|A m_k- e_k\| \leq  \varepsilon 
\end{equation}
\end{bgd}
The main theorem concerning SPAI is stated as follows: 
%We now derive bounds for the singular values and the condition number of AM.
\begin{theorem}
The singular values of \(A M\) are clustered at 1 and lie inside the
interval \([1-\delta, 1+\delta],\) with \(\delta=\sqrt{n} \varepsilon(2+\sqrt{n} \varepsilon) .\) Furthermore, if \(\delta<1,\) then the
condition number of \(A M\) satisfies

\begin{equation}
\operatorname{cond}_{2}(A M) \leq \sqrt{\frac{1+\delta}{1-\delta}}
\end{equation}


\end{theorem}

\begin{rem}
We believe that there is a minor typo in the statement of the original theorem as singular values lie inside the interval  \([\sqrt{1-\delta}, \sqrt{1+\delta}]\), which is evident from the nature of the bound on the condition number. 
\end{rem}

We first derive some useful lemma's that will be needed for the proof and recall Gershgorin's circle theorem. 

\begin{lemma}\label{lma:norms}
Let \(p=\max _{1 \leq k \leq n}\left\{\text { number of nonzero elements of } r_{k}\right\} .\) Then
\begin{align}
\|A M-I\|_{F} \leq \sqrt{n} \varepsilon \label{lem1a} \\
\|A M-I\|_{2} \leq \sqrt{n} \varepsilon \label{lem1b} \\
\|A M-I\|_{1}  \leq \sqrt{p} \varepsilon \label{lem1c}
\end{align}
\end{lemma}

\begin{proof}
\begin{align}
\|A M-I\|_{F}^{2}=\sum_{k=1}^{n}\left\|(A M-I) e_{k}\right\|_{2}^{2} \; \overset{\eqref{eq:conv}}{\leq} \; \sum_{k=1}^{n} \varepsilon^{2}=n \varepsilon^{2}
\end{align}

\begin{align}
\|A M-I\|_{2} &=\max _{\|x\|_{2}=1}\|(A M-I) x\|_{2} =\max _{\|x\|_{2}=1}\left\|\sum_{k=1}^{n} x_{k}(A M-I) e_{k}\right\|_{2} \\
&\leq \max _{\|x\|_{2}=1}\|x\|_{1} \varepsilon \overset{(a)}{\leq} \sqrt{n} \varepsilon
\end{align}
In the above, (a) essentially follows from Cauchy-Schwarz and the detailed proof can be found in the Section \ref{sec:csi} of the Appendix.
\begin{align}
\|A M-I\|_{1} &\overset{\text{def}}{=}\max_{1 \leq k \leq n}\left\|A m_k - e_k\right\|_{1} \label{l1matrix} \\
&\leq \sqrt{p} \max_{1 \leq k \leq n}\left\|A m_k - e_k\right\|_{2} 
\end{align}
The last inequality comes from utilizing the fact that $r_k = A m_k -e_k$ has atmost $p$ non-zero elements, along with Cauchy-Schwarz like proof of (a).

\end{proof}

\begin{lemma}[Gershgorin, 1931]\label{lma:gersh}
	
Let A be an $n \times n$ matrix with entries in $\mathbb{C}$. The eigenvalues of $A$ belong to the union of Gershgorin disks $(D_i)_{i=1}^n$,
\begin{equation}\label{gers1}
D_{i}=\left\{z \in \mathbb{C} \, | \, | z-a_{i i} | \leq R_{i}\right\},\,  \text{where} \; R_{i}=\sum_{j \neq i}\, \left|a_{i j}\right| \; \text{and } \, a_{i j} \; \text{denote the entries of }A.  \\
\end{equation}

Corollary: The eigenvalues of A must also lie within the Gershgorin discs $C_j$ corresponding to the columns of A. \\


\end{lemma}

\begin{lemma}\label{lma:cond}
The eigenvalues $\lambda_k$ of $AM$ are clustered at 1 and lie inside a circle of radius $\sqrt{p} \varepsilon$
\end{lemma}

\begin{proof}
	
Let $Q R Q^{T}$ be a Schur decomposition of $A M-I$. Then:

\begin{equation}
\sum_{k=1}^{n}\left|1-\lambda_{k}\right|^{2} \overset{(b)}{=} \|\operatorname{diag}(R)\|_{2}^{2} \leq\|R\|_{F}^{2} \overset{(c)}{=} \|A M-I\|_{F}^{2} \overset{\eqref{lem1a}}{\leq} n \varepsilon^{2}
\end{equation}
The first equality labeled (b) holds as $R$ and $ A M -I$ are similar matrices and thus have same eigenvalues. Note $R$ is an upper triangular matrix and contains eigenvalues on its diagonal.
Further, (c) follows from the orthogonal invariance of the Frobenius norm and the proof of which can be found in Section \ref{sec:ortho} of the Appendix. \\

Overall, we have:
\begin{equation}
\frac{1}{n} \sum_{k=1}^{n}\left|1-\lambda_{k}\right|^{2} \leq \varepsilon^{2}
\end{equation}
This implies that the eigenvalues are clustered around 1. The part about them lying inside a circle is shown ahead. Let's denote the matrix $A M$ by $G$ and $A M - I$ by $\tilde{G}$. The individual entries of these matrices are denoted by the respective lowercase letters. Then, we can write

\begin{equation}\label{gers2}
\left|\lambda_{k} - 1 \right| = \left|\lambda_{k} - \dk  + \dk - 1 \right| \leq \left|\lambda_{k} - \dk \right|  + \left|\dk - 1 \right| %\eq = \left|\lambda_{k} - \dk \right|  + \left|\tilde{g}_{kk}\right|
\end{equation}

Applying corollary of Lemma \ref{lma:gersh}, i.e. in particular using the Gershgorin's disc $C_k$ corresponding to column $k$, we obtain:

\begin{equation}\label{gers3}
\left|\lambda_{k} - \dk \right| \leq \sum_{j \neq k}\, \left|g_{j k}\right|
\end{equation}

Since $ \dktil = \dk - 1 $ and $\tilde{g}_{j k} = g_{j k}$ (for $j\neq k$), from equations \eqref{gers2} and \eqref{gers3}, we have:  

\begin{equation}\label{gers4}
\left|\lambda_{k} - 1 \right| \leq \sum_{j \neq k}\, \left|\tilde{g}_{j k}\right| + \left|\dktil\right| = \sum_{j }\, \left|\tilde{g}_{j k}\right| = \|A m_k -e_k\|_{1} \overset{\eqref{l1matrix}}{\leq} \|A M-I\|_{1}
\end{equation}

The last inequality follows from the definition of 1-norm of a matrix \eqref{l1matrix}. Thus from the above equation \eqref{gers4} and equation \eqref{lem1c} in Lemma \ref{lma:norms}, we conclude that eigenvalues of the matrix $A M$ satisfy $ \left|\lambda_{k} - 1 \right| \leq \sqrt{p} \varepsilon$ and hence lie inside the required circle.
\end{proof}


%$$
%\square
%$$
\vspace{2em}
\begin{proof}[\textbf{Proof of the main theorem.}] 

Now, with these lemma's in place let's get back to proving the original theorem. \\

Since the singular values of $AM$ are the square roots of the eigenvalues of $A M M^{T} A^{T} $, 


\begin{align}\label{eq:e1}
\left\|I-A M M^{T} A^{T}\right\|_{2} &= \left\|I+(I-A M) M^{T} A^{T}-M^{T} A^{T}\right\|_{2} \\ & =\left\|(I-A M)^{T} + (I-A M)M^{T}A^{T}\right\|_{2}
\end{align}
\\
Next we use the triangle inequality as well as the sub-multiplicativity of the 2-norm.
\\
\begin{align}\label{eq:e2}
\left\|(I-A M)^{T} + (I-A M)M^{T}A^{T}\right\|_{2}
& \leq \left\|I-A M\right\|_{2} + \left\|I-A M\right\|_{2}\left\|AM\right\|_{2} \\
& \overset{\eqref{lem1b}}{\leq} \sqrt{n} \varepsilon\left(1+\|A M\|_{2}\right)
\end{align}

Further, rewrite the right hand side of equation \eqref{eq:e2}, apply triangle inequality, and then combine with equation \eqref{eq:e1}, to get : 
\begin{align}
\left\|I-A M M^{T} A^{T}\right\|_{2} & \leq \sqrt{n} \varepsilon\left(1+\|A M-I+I\|_{2}\right) \leq \sqrt{n} \varepsilon\left(1+1+\|A M-I\|_{2}\right) \\
&= \sqrt{n} \varepsilon\left(2 +\|A M-I\|_{2}\right)  \overset{\eqref{lem1b}}{\leq} \sqrt{n} \varepsilon(2+\sqrt{n} \varepsilon) \overset{\text{def}}{=} \delta
\end{align}
Thus we have the bound on the condition number of the matrix $AM$ as follows: 
\begin{equation}
\operatorname{cond}_{2}(A M) = \frac{\sigma_{\text{max}}(AM)}{\sigma_{\text{min}}(AM)} =  \sqrt{\frac{\lambda_{\text{max}}(AM M^{T} A^{T})}{\lambda_{\text{min}}(AM M^{T} A^{T})}} \leq \sqrt{\frac{1+\delta}{1-\delta}}
\end{equation}

Finally, applying Lemma \ref{lma:cond} to the matrix $ A M M^{T} A^{T} $ instead of $ A M$ we conclude that the singular values are indeed clustered at 1 and lie inside the interval \([\sqrt{1-\delta}, \sqrt{1+\delta}]\).
\end{proof}

\newpage
\begin{section}{Experiments: SPAI}

\subsection{Quality of preconditioning}

\begin{figure}[h]
\centering
 \includegraphics[width=0.75\linewidth]{grote2.png}
\caption{Results as reported in Grote and Huckle \cite{grote}.}
	\label{fig:groteh}
	
\end{figure}
	
%\includegraphics[scale=.5]{grote2.png}

Here, our objective is to reproduce the results of SPAI (c.f. Figure \ref{fig:groteh}) on the Orsirr2 matrix. Table \ref{tab:grote2} shows our results for the same. We observe that our results exactly the same as the ones from \cite{grote}. 
	\begin{table}[h]
	\centering
	\begin{tabular}{@{}cccccc@{}}
		\toprule
		$\varepsilon$ &    $||AM-I||_F$ &  $||AM-I||_2$ &   $||AM-I||_1$ &  $\operatorname{cond}_2(AM)$   & $\frac{{nz}(M)}{{nz}(a)}$ \\
		\midrule
		0.60 & 14.26 &  1.02  &  1.22 & 1989.98  &  0.32 \\
		
		0.50 & 11.29 &  1.08  &  1.56 & 210.79  &  0.61 \\
		
		0.40 &  8.97 &  1.05  &  1.56 & 74.15  &   0.89 \\
		
		0.30  & 7.12  & 1.00  &  1.76 & 35.13  &  1.53 \\
		
		0.20 &  4.81 &  0.98  &  1.72 &  12.82 &   3.41 \\
		\bottomrule
	\end{tabular}
	\caption{Our results for SPAI on the Orsirr2 matrix using identical parameters.}
	\label{tab:grote2}
\end{table}



\subsection{Performance with linear system solvers}
Next, we aim to verify the numerical results when the preconditioner is used along with a linear system solver, in particular BICGSTAB. We compare the number of iterations\footnote{Note that MATLAB implementation of BICGSTAB sometimes reports fractional iterations to indicate ``convergence half way through an iteration''} required by BICGTSTAB for convergence, the results of which can be found in Table \ref{tab:grote1}. 
\begin{table}[h]
	\centering
	\begin{tabular}{@{}ccc@{}}
		\toprule
		$\varepsilon$ & Bi-CGSTAB (ours) & Bi-CGSTAB (original)\\
		\midrule
		0.6    &   172.5       & 223\\     
		0.5    &   59.5        & 68\\    
		0.4    &   33.5        &47 \\    
		0.3    &   29.5        & 32 \\    
		0.2    &   15.5 & 18 \\
		\bottomrule
	\end{tabular}
	\caption{}
	\label{tab:grote1}
\end{table}

We observe that we get better results, i.e., lower number of iterations to converge. This is probably because of the improvements in implementation of the solver over time. \\

Additionally, we compare to results from Benzi and Tuma \cite{benzi} for a similar set of experiments on the performance of the preconditioner with a linear system solver (BICGSTAB again). The authors \cite{benzi} report results on two datasets: 3DCD and Memplus, but the former is not publicly available. Hence, we stick to Memplus and the results of which can be found in Table \ref{tab:benzi}.

\begin{table}[h]
	\centering
	\begin{tabular}{@{}cccc@{}}
		\toprule
		SPAI error threshold & BICGSTAB tolerance & \# Iterations & Relative residual  \\ \midrule
		\multirow{4}{*}{0.5}& 1e-08 & 9.0 & 1e-08 \\
		& 1e-12 & 93.0 & 4e-13 \\
		& 1e-16 & 156.5 & 2e-16 \\
		(original)			& NA & 140.0 & NA \\
		\midrule
		\multirow{4}{*}{0.3} & 1e-08	& 8.5	& 9e-09 \\
		& 1e-12	& 73.5	& 1e-12 \\
		& 1e-16	& 122.5	& 5e-16 \\
		(original)					& NA & 117.0 & NA \\
		\bottomrule
	\end{tabular}
	\caption{Results of solving a linear system (with BICGSTAB) for the Memplus matrix using the sparse approximate inverse with those mentioned in Benzi and Tuma \cite{benzi}.}
	\label{tab:benzi}
\end{table}

An important practical aspect that we couldn't find from the paper was the value of BICGSTAB tolerance employed by them. Hence, we show the results at various values of this tolerance, and we find that for $1e-16$ the results closely match and for values larger than that BICGSTAB converges much faster. 

\paragraph{Summary.} Hence, we find that the results of SPAI can be reproduced fairly well across various experimemts. 
\end{section}
\newpage
\begin{section}{MR algorithm}
	
	Chow and Saad \cite{chow} also propose an algorithm based on minimal residual iterations (aka MR) to find a sparse approximate inverse via minimizing the Frobenius norm.\\
	
	Like SPAI \cite{grote} (which also aims to minimize Frobenius norm), this algorithm considers each column of $M$ independently. Thus, for each column it runs $n_{i}$ iterations to minimize the residual, similar in spirit to a descent method, while also performing numerical dropping to enforce sparsity in $M$. 
	
	\subsection{Derivation of MR}
	
	\paragraph{Initialization.}
	$M$ is initialized by either a scaled identity matrix or scaled transpose of the given matrix $A$. In other words, $M = \alpha I$ or $M =\alpha A^{T}$. The latter has the benefit that preconditioner is of a similar sparsity structure as the given matrix $A$ and this is the one used for numerical experiments later.  
	
	\paragraph{Update strategy.}
	At each iteration for the column $m_{j}$, it computes the current residual $r_j$ and the corresponding update  $\hat{m}_{j}$ as follows:
	\begin{equation}
	r_{j} :=e_{j}-A m_{j}, \quad \text{and} \quad \hat{m}_{j} :=m_{j}+\alpha_{j} r_{j}.
	\end{equation}

	Then, the update $\hat{m}_{j}$ is carried out in the direction of $r_j$ (the current residual), so as to minimize the resulting residual at next step. This involves solving a one-dimensional optimization problem to compute the optimal step size $\alpha_{j}$. This procedure is then repeated until $j = n_{i}$ and  is the gist of the algorithm, besides the numerical dropping discussed later. 
	
	\begin{lemma}
Under the update rule $\hat{m}_{j} :=m_{j}+\alpha_{j} r_{j}$, 	the optimal step size $\alpha_{j}$ with respect to the next residual is 
	
	\begin{equation}
	\alpha_{j} :=\frac{\langle r_{j}, A r_{j}\rangle}{\langle A r_{j}, A r_{j}\rangle}
	\end{equation}
	
\end{lemma}

\begin{proof}
		For the sake of clarity, we drop the subscript $j$ corresponding to the column index in the proof ahead. 
		Let $r, e, m$ be vectors s.t. $r = e - Am$. If we want to minimize the norm of $\hat{r}$; where $\hat{r} = e - A\hat{m}$, s.t. $\hat{m}$ lies along the path $m + \alpha r$. We need to show that the optimal step size is $$\alpha = \frac{\langle\,r,Ar\rangle}{\|Ar\|_{2}^{2}}$$
	
		We compute the gradient of $\|\hat{r}\|_{2}$ w.r.t. $\alpha$ to find the optimum step size. 
	
	\begin{equation}
	\begin{aligned}
	\frac{\partial \|\hat{r} \|_{2}^{2}}{\partial \alpha} 
	& = \frac{\partial \|e - A\hat{m}\|_{2}^{2}}{\partial \alpha} \\
	& = \frac{\partial \|e - A(m + \alpha r)\|_{2}^{2}}{\partial \alpha} \\
	& = 2\alpha \|Ar\|_{2}^{2}  + 2(Am)^{T}Ar - 2e^{T}Ar \\
	& = 2\alpha \|Ar\|_{2}^{2}  + 2(Am - e)^{T}Ar \\
	& = 2\alpha \|Ar\|_{2}^{2}  - 2r^{T}Ar \\
	\\
	\end{aligned}
	\end{equation}
	
	By setting the derivative to $0$ we find that $\alpha = \frac{\langle\,r,Ar\rangle}{\|Ar\|_{2}^{2}}$\footnote{This is a point of minima as the second derivative is clearly positive.}.
\end{proof}	

	
	
	\subsection{Numerical dropping}
	The goal of numerical dropping is to keep $M$ sparse. Let's designate by $\text{lfil}$, the number of entries to keep in each column of $M$ during this procedure. In \cite{chow}, this is done by computing for each entry $i$ of column $j$ the following value, 
	

	\begin{equation}\label{eq:rho}
	\rho_{i j}=-2 m_{i j}\langle e_{i}, A^{T} r_{j}\rangle+m_{i j}^{2}.
	\end{equation}

	%Here, the $\rho_{i j}$ measures the amount of reduction in the residual by considering entry $i$. We then keep the $\text{lfil}$ entries which result in the maximum decrease.

The reasoning for having such a criteria is based on the following equation \eqref{eq:pert} derived via a perturbation analysis.
Consider that $\left\|\hat{r}_{j}\right\|_{2}^{2}$ denotes the residual of a perturbed $\hat{m}_{j} = m_j + d$, where $d = m_{ij} e_j$.

\begin{equation}\label{eq:pert}
\left\|\hat{r}_{j}\right\|_{2}^{2}-\left\|r_{j}\right\|_{2}^{2}= \rho_{i j} = -2 m_{i j}\langle e_{i}, A^{T} r_{j}\rangle +m_{i j}^{2}
\end{equation}

This intuitively means that $\rho_{i j}$ measures the amount of reduction in the residual by considering entry $i$. We then keep the $\text{lfil}$ entries which result in the maximum decrease.
%This gives rise to the following strategy:
%pick the $\text{lfil}$ elements with the highest $\rho_{i j}$ value, as if these are perturbed, they will cause the residual to increase the most, and therefore it would make sense to keep them. 
	
	
\end{section}


\newpage

\begin{section}{Experiments: MR}
	

\begin{table}[h]
	\centering
\begin{tabular}{@{}lc|cc|cccccc@{}}
	\toprule
	Source & Matrix &    $\text{lfil}$ &  $n_{i}$ &   $1$ &  $2$ &  $3$ & $4$ &  $5$ \\ \midrule
	\multirow{3}{*}{Ours} & SHERMAN1 & \multirow{3}{*}{10} & \multirow{3}{*}{1}  & 210 & 81 & 80 & 48 & 4  \\
	
	& SHERMAN3 &  &  & 546 & 48 & 306 & 65 & 156  \\

	& SAYLR3 & & & 240 & 81 & 80 & 48 & 4  \\
		\midrule
	\multirow{3}{*}{Original} & SHERMAN1 & \multirow{3}{*}{10} & \multirow{3}{*}{1} & 224 & 187 & 96 & 74 & 60  \\

& SHERMAN3 &  &  & 499 & 363 & 239 & 192 & 148  \\

& SAYLR3 & & & 223 & 188 & 96 & 74 & 60  \\
	\bottomrule
	\end{tabular}
	\caption{}
\label{tab:chow}
\end{table}
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}


\end{section}

\clearpage
\bibliographystyle{unsrt}
\bibliography{references}

\section{Appendix}

\subsection{Orthonal invariance of the Frobenius norm}\label{sec:ortho}
Let us consider a matrix $R$ and on applying orthogonal matrices on both sides in the form of $QRQ^{T}$. We show that the Frobenius norm remains invariant to such an orthogonal transformation. \\

The following chain of equalities follows by using the orthogonality of
Q as well as the cyclical property of the trace.
\begin{equation}
\begin{aligned}
\|QRQ^{T} \|_{F}^{2}
& =  tr((QRQ^{T})^{T}QRQ^{T}) \\
& =  tr(QR^{T}Q^{T}QRQ^{T}) \\
& =  tr(QR^{T}RQ^{T}) \\
& =  tr(R^{T}RQ^{T}Q) \\
& =  tr(R^{T}R) \\
& =  \|R\|_{F}^{2} 
\end{aligned}
\end{equation}
	
\subsection{Some corollaries of the Cauchy Schwarz inequality} \label{sec:csi}

Cauchy Schwarz states that:

\begin{equation}
\langle x,y\rangle \leq \|x\|_{2} \, \|y\|_{2}
\end{equation}

If $x_i \geq 0, \, \forall i$, we can set $y= (1, 1, ..., 1)$ to get the following inequality:.
%Let $x = (x_{1}, ..., x_{n})$ and $y = (1, 1, ..., 1)$

\begin{equation}
\sum_{i=1}^{n} x_{i} = \langle x,y\rangle  \leq  \|x\|_{2}\, \|y\|_{2} = \sqrt{n}\|x\|_{2}
\end{equation}

In fact, the above holds without loss of generality by considering $y_{i} = sign(x_{i})$. 

\begin{equation}
\sum_{i=1}^{n} \left|x_{i}\right| = \langle x,y\rangle =  \|x\|_{1}\leq \sqrt{n}\|x\|_{2}
\end{equation}	

%TODO
% Make it lemma 1.1 and 1.2 and so on! 
\end{document}