%%% document layout
\documentclass[paper=A4, fontsize=11pt]{scrartcl}
\setlength\parindent{0pt}
\usepackage[margin=1in]{geometry}

%%% packages
\usepackage{amsmath,amssymb,amsthm}
\usepackage{bbm}
\usepackage{complexity}
\usepackage{tcolorbox}
\usepackage{mathrsfs}

\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{multirow}


%%% hacks
\setlength\parindent{0pt}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}
\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}

%%% macros
\let\oldpr\Pr
\renewcommand{\Pr}[1]{\oldpr\left( #1 \right)}
\newcommand{\Prsub}[2]{\oldpr_{#1}\left( #2 \right)}
\newcommand{\EX}[1]{{{\mathbb{E}}\left[#1\right]}}
\newcommand{\EXSUB}[2]{{{\mathbb{E}}_{#1}\left[#2\right]}}
\newcommand{\VAR}[1]{\text{Var}\left[#1\right]}
\newcommand{\COVAR}[2]{\text{Cov}\left[#1,\, #2\right]}
\newcommand{\ZO}{\{0, \, 1\}}
\newcommand{\median}[1]{\text{med}\left\lbrace #1\right\rbrace}
\newcommand{\order}[1]{\mathcal{O}\left(#1\right)}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\indicator}[1]{\mathbf{1}_{\left[#1\right]}}
\newcommand{\twosum}[2]{\sum_{\substack{#1\\#2}}}
\newcommand{\support}[1]{\textup{sup}\left(#1\right)}
\newcommand{\partspace}{\vspace{.3cm}}
\newcommand{\exspace}{\vspace{.8cm}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%%% envs def
\newtheorem{lemma}{Lemma}[section]
%\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}[section]
%\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

%\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{bgd}{Background}

\newcommand{\dk}{g_{kk}} % D_kk
\newcommand{\dktil}{\tilde{g}_{kk}} % Dtil_kk


%%% title
\title{Sparse Approximate Inverse Preconditioner}
\subtitle{Computational Linear Algebra}
\author{Sidak Pal Singh}

\begin{document}
	\maketitle
	%\textbf{Disclaimer: } This project follows closely a project given in a different class covering convex optimization.
	
	\section{Abstract}
	
	In this project, the aim is to construct a sparse preconditioner M for a given sparse matrix A. More specifically, we look for a right preconditioner M such that $\|I-A M\|_{F}$ is minimized. \\
	One of the most successful general-purpose methods for achieving this tasks is SPAI by Grote and Huckle [3]. We will derive their algorithm along with a theorem 
	
	
	\section{Introduction}
	
	intro of sparse inverse problem
	
	\newpage
	
%	\section{Prelude: A small lemma}
%	
%	Before we start our journey into the sparse inverse solvers, we introduce a small lemma:
%	
%	\begin{lemma}
%		
%		Let $A$ be a matrix and $r, e, m$ be vectors s.t. $r = e - Am$. If we want to minimize the norm of $\hat{r}$; where $\hat{r} = e - A\hat{m}$, s.t. $\hat{m}$ lies along the path $m + \alpha r$. \\
%		Then, the optimal step size is $\alpha = \frac{\langle\,r,Ar\rangle}{\|Ar\|_{2}^{2}}$
%		\\\\
%		Proof 
%		\\\\
%		We compute the gradient of $\|\hat{r}\|_{2}$ w.r.t. $\alpha$ to find the optimum step size. 
%		
%		
%		
%		
%		\begin{equation}
%		\begin{aligned}
%		\frac{\partial \|\hat{r} \|_{2}^{2}}{\partial \alpha} 
%		& = \frac{\partial \|e - A\hat{m}\|_{2}^{2}}{\partial \alpha} \\
%		& = \frac{\partial \|e - A(m + \alpha r)\|_{2}^{2}}{\partial \alpha} \\
%		& = 2\alpha \|Ar\|_{2}^{2}  + 2(Am)^{T}Ar - 2e^{T}Ar \\
%		& = 2\alpha \|Ar\|_{2}^{2}  + 2(Am - e)^{T}Ar \\
%		& = 2\alpha \|Ar\|_{2}^{2}  - 2r^{T}Ar \\
%		\\
%		\end{aligned}
%		\end{equation}
%		
%		By setting the derivative to $0$ we find that $\alpha = \frac{\langle\,r,Ar\rangle}{\|Ar\|_{2}^{2}}$
%		
%		
%		
%		
%	\end{lemma}
%	
%	\begin{lemma}
%		Let $A$ be a matrix and $r, e, m$ be vectors as before. 
%		
%		Then
%		
%		\begin{equation}
%		\min _{\mu_{j}}\left\|r+\mu_{j} A e_{j}\right\|_{2}
%		\end{equation}
%		
%		Has the following solution
%		
%		\begin{equation}
%		\mu_{j}=-\frac{r^{T} A e_{j}}{\left\|A e_{j}\right\|_{2}^{2}}
%		\end{equation}
%		
%		Proof
%		\\
%		We can use verbatim the proof of lemma 1.
%	\end{lemma}
%	
%	\newpage
	
\section{SPAI}

The goal is to efficiently find a sparse inverse of $A$, the SPAI algorithm does so by minimizing the Frobenious norm as it is more tractable and it also leads to inherent parallelism given that the columns of $M$ are independent of one another.

\begin{equation}
\|A M-I\|_{F}^{2}=\sum_{k=1}^{n}\left\|(A M-I) e_{k}\right\|_{2}^{2}
\end{equation}

Hence, it suffices to solve the following problem independently for each $k$


\begin{equation}
\min _{m_{k}}\left\|A m_{k}-e_{k}\right\|_{2}, \quad k=1, \ldots, n
\end{equation}


We define the following objects as is done in the original paper. (To take advantage of the sparsity of A)\\

Let \(\mathcal{J}\) be the set of indices \(j\) such that \(m_{k}(j) \neq 0\) We denote the reduced vector of unknowns \(m_{k}(\mathcal{J})\) by \(\hat{m}_{k} .\) Next, let \(\mathcal{I}\) be the set of
indices \(i\) such that \(A(i, \mathcal{J})\) is not identically zero. This enables us to eliminate all zero rows in the submatrix \(A( ., \mathcal{J}) .\) We denote the resulting submatrix \(A(\mathcal{I}, \mathcal{J})\) by by
\(\hat{A}\) . Similarly, we define \(\hat{e}_{k}=e_{k}(\mathcal{I}) .\) If we now set \(n_{1}=|\mathcal{I}|\) and \(n_{2}=|\mathcal{J}|,\) we see that
solving \((4)\) for \(m_{k}\) is equivalent to solving

\begin{equation}
\min _{\hat{m}_{k}}\left\|\hat{A} \hat{m}_{k}-\hat{e}_{k}\right\|_{2}
\end{equation}



\begin{equation}
\hat{A}=Q\left(\begin{array}{l}{R} \\ {0}\end{array}\right)
\end{equation}

where \(R\) is a nonsingular upper triangular \(n_{2} \times n_{2}\) matrix. If we let \(\hat{c}=Q^{T} \hat{e}_{k},\) the
solution of  is

\begin{equation}
\hat{m}_{k}=R^{-1} \hat{c}\left(1 : n_{2}\right)
\end{equation}

Solve (5) for every column of M to get an approximate inverse. The next 
step is to increase the sparsity structure to improve upon the previous solution.

The idea is to add indices that will lead to best reduction in error. We recall that the current error is \(\|A M-I\|_{F},\), in other words to \(\left\|A m_{k}-e_{k}\right\|_{2}\) for each \(k=1, \ldots, n .\) We recall that \(m_{k}\) is the optimal solution of the least squares problem (4), and we denote its residual by

\begin{equation}
r=A( ., \mathcal{J}) \hat{m}_{k}-e_{k}
\end{equation}

must be included in \(\mathcal{L}\) since \(r(k)\) is then equal to \(-1 .\) To every \(\ell \in \mathcal{L}\) corresponds an
index set \(\mathcal{N}_{\ell},\) which consists of the indices of the nonzero elements of \(A(\ell, .)\) that are


\begin{equation}
\tilde{\mathcal{J}}=\bigcup_{\ell \in \mathcal{L}} \mathcal{N}_{\ell}
\end{equation}

We choose the best move

\begin{equation}
\min _{\mu_{j}}\left\|r+\mu_{j} A e_{j}\right\|_{2}
\end{equation}

To find the optimal step, we differentiate and set the derivative to 0. 

\begin{equation}
\begin{aligned}
\frac{\partial \left\|r+\mu_{j} A e_{j}\right\|_{2}}{\partial \mu_{j}} 
& = 2r^{T} A e_{j} + 2\mu_{j}\|A e_{j}\|_{2}^{2}\\
\end{aligned}
\end{equation}

Setting the derivative to $0$ yields

\begin{equation}
\mu_{j}=-\frac{r^{T} A e_{j}}{\left\|A e_{j}\right\|_{2}^{2}}
\end{equation}

\begin{equation}
\rho_{j}^{2}=\|r\|_{2}^{2}-\frac{\left(r^{T} A e_{j}\right)^{2}}{\left\|A e_{j}\right\|_{2}^{2}}
\end{equation}

\begin{equation}
0=r(\mathcal{L})^{T} A(\mathcal{L}, \tilde{\mathcal{J}})
\end{equation}

\newpage
	
	
\section{The Grote and Huckle theorem}

\begin{bgd} Assume $M$ is the approximate inverse of $A$ obtained from SPAI algorithm, and let $m_k$ denote the $k^{\text{th}}$ column and $r_k$ be the corresponding residual. We assume that at convergence, 
\begin{equation}\label{eq:conv}
\| r_k\| = \|A m_k- e_k\| \leq  \varepsilon 
\end{equation}
\end{bgd}
The main theorem concerning SPAI is stated as follows: 
%We now derive bounds for the singular values and the condition number of AM.
\begin{theorem}
The singular values of \(A M\) are clustered at 1 and lie inside the
interval \([1-\delta, 1+\delta],\) with \(\delta=\sqrt{n} \varepsilon(2+\sqrt{n} \varepsilon) .\) Furthermore, if \(\delta<1,\) then the
condition number of \(A M\) satisfies

\begin{equation}
\operatorname{cond}_{2}(A M) \leq \sqrt{\frac{1+\delta}{1-\delta}}
\end{equation}


\end{theorem}

\begin{rem}
We believe that there is a minor typo in the statement of the original theorem as singular values lie inside the interval  \([\sqrt{1-\delta}, \sqrt{1+\delta}]\), which is evident from the nature of the bound on the condition number. 
\end{rem}

We first derive some useful lemma's that will be needed for the proof and recall Gershgorin's circle theorem. 

\begin{lemma}\label{lma:norms}
Let \(p=\max _{1 \leq k \leq n}\left\{\text { number of nonzero elements of } r_{k}\right\} .\) Then
\begin{align}
\|A M-I\|_{F} \leq \sqrt{n} \varepsilon \label{lem1a} \\
\|A M-I\|_{2} \leq \sqrt{n} \varepsilon \label{lem1b} \\
\|A M-I\|_{1}  \leq \sqrt{p} \varepsilon \label{lem1c}
\end{align}
\end{lemma}

\begin{proof}
\begin{align}
\|A M-I\|_{F}^{2}=\sum_{k=1}^{n}\left\|(A M-I) e_{k}\right\|_{2}^{2} \; \overset{\eqref{eq:conv}}{\leq} \; \sum_{k=1}^{n} \varepsilon^{2}=n \varepsilon^{2}
\end{align}

\begin{align}
\|A M-I\|_{2} &=\max _{\|x\|_{2}=1}\|(A M-I) x\|_{2} =\max _{\|x\|_{2}=1}\left\|\sum_{k=1}^{n} x_{k}(A M-I) e_{k}\right\|_{2} \\
&\leq \max _{\|x\|_{2}=1}\|x\|_{1} \varepsilon \overset{(a)}{\leq} \sqrt{n} \varepsilon
\end{align}
In the above, (a) essentially follows from Cauchy-Schwarz and the detailed proof can be found in the Section \ref{sec:csi} of the Appendix.
\begin{align}
\|A M-I\|_{1} &\overset{\text{def}}{=}\max_{1 \leq k \leq n}\left\|A m_k - e_k\right\|_{1} \label{l1matrix} \\
&\leq \sqrt{p} \max_{1 \leq k \leq n}\left\|A m_k - e_k\right\|_{2} 
\end{align}
The last inequality comes from utilizing the fact that $r_k = A m_k -e_k$ has atmost $p$ non-zero elements, along with Cauchy-Schwarz like proof of (a).

\end{proof}

\begin{lemma}[Gershgorin, 1931]\label{lma:gersh}
	
Let A be an $n \times n$ matrix with entries in $\mathbb{C}$. The eigenvalues of $A$ belong to the union of Gershgorin disks $(D_i)_{i=1}^n$,
\begin{equation}\label{gers1}
D_{i}=\left\{z \in \mathbb{C} \, | \, | z-a_{i i} | \leq R_{i}\right\},\,  \text{where} \; R_{i}=\sum_{j \neq i}\, \left|a_{i j}\right| \; \text{and } \, a_{i j} \; \text{denote the entries of }A.  \\
\end{equation}

Corollary: The eigenvalues of A must also lie within the Gershgorin discs $C_j$ corresponding to the columns of A. \\


\end{lemma}

\begin{lemma}\label{lma:cond}
The eigenvalues $\lambda_k$ of $AM$ are clustered at 1 and lie inside a circle of radius $\sqrt{p} \varepsilon$
\end{lemma}

\begin{proof}
	
Let $Q R Q^{T}$ be a Schur decomposition of $A M-I$. Then:

\begin{equation}
\sum_{k=1}^{n}\left|1-\lambda_{k}\right|^{2} \overset{(b)}{=} \|\operatorname{diag}(R)\|_{2}^{2} \leq\|R\|_{F}^{2} \overset{(c)}{=} \|A M-I\|_{F}^{2} \leq n \varepsilon^{2}
\end{equation}
The first equality labeled (b) holds as $R$ and $ A M -I$ are similar matrices and thus have same eigenvalues. Note $R$ is an upper triangular matrix and contains eigenvalues on its diagonal.
Further, (c) follows from the orthogonal invariance of the Frobenius norm and the proof of which can be found in Section \ref{sec:ortho} of the Appendix. \\

Overall, we have:
\begin{equation}
\frac{1}{n} \sum_{k=1}^{n}\left|1-\lambda_{k}\right|^{2} \leq \varepsilon^{2}
\end{equation}
This implies that the eigenvalues are clustered around 1. The part about them lying inside a circle is shown ahead. Let's denote the matrix $A M$ by $G$ and $A M - I$ by $\tilde{G}$. The individual entries of these matrices are denoted by the respective lowercase letters. Then, we can write

\begin{equation}\label{gers2}
\left|\lambda_{k} - 1 \right| = \left|\lambda_{k} - \dk  + \dk - 1 \right| \leq \left|\lambda_{k} - \dk \right|  + \left|\dk - 1 \right| %\eq = \left|\lambda_{k} - \dk \right|  + \left|\tilde{g}_{kk}\right|
\end{equation}

Applying corollary of Lemma \ref{lma:gersh}, i.e. in particular using the Gershgorin's disc $C_k$ corresponding to column $k$, we obtain:

\begin{equation}\label{gers3}
\left|\lambda_{k} - \dk \right| \leq \sum_{j \neq k}\, \left|g_{j k}\right|
\end{equation}

Since $ \dktil = \dk - 1 $ and $\tilde{g}_{j k} = g_{j k}$ (for $j\neq k$), from equations \eqref{gers2} and \eqref{gers3}, we have:  

\begin{equation}\label{gers4}
\left|\lambda_{k} - 1 \right| \leq \sum_{j \neq k}\, \left|\tilde{g}_{j k}\right| + \left|\dktil\right| = \sum_{j }\, \left|\tilde{g}_{j k}\right| = \|A m_k -e_k\|_{1} \overset{\eqref{l1matrix}}{\leq} \|A M-I\|_{1}
\end{equation}

Thus from the above equation \eqref{gers4} and equation \eqref{lem1c} in Lemma \ref{lma:norms}, we conclude that eigenvalues of the matrix $A M$ satisfy $ \left|\lambda_{k} - 1 \right| \leq \sqrt{p} \varepsilon$ and hence lie inside the required circle.
\end{proof}


%$$
%\square
%$$
\vspace{2em}
\begin{proof}[\textbf{Proof of the main theorem.}] 

Now, with these lemma's in place let's get back to proving the original theorem. \\

Since the singular values of $AM$ are the square roots of the eigenvalues of $A M M^{T} A^{T} $, 


\begin{align}
\left\|I-A M M^{T} A^{T}\right\|_{2} &= \left\|I+(I-A M) M^{T} A^{T}-M^{T} A^{T}\right\|_{2} \\ & =\left\|(I-A M)^{T} + (I-A M)M^{T}A^{T}\right\|_{2}
\end{align}
\\
Next we use the triangle inequality as well as the sub-multiplicativity of the 2-norm.
\\
\begin{align}
\left\|(I-A M)^{T} + (I-A M)M^{T}A^{T}\right\|_{2}
& \leq \left\|I-A M\right\|_{2} + \left\|I-A M\right\|_{2}\left\|AM\right\|_{2} \\
& \leq \sqrt{n} \varepsilon\left(1+\|A M\|_{2}\right)
\end{align}

Rewriting the right hand side as follows: 
\begin{align}
\sqrt{n} \varepsilon\left(1+\|A M-I+I\|_{2}\right) &\leq \sqrt{n} \varepsilon\left(1+1+\|A M-I\|_{2}\right) \\
&= \sqrt{n} \varepsilon\left(2 +\|A M-I\|_{2}\right)  \leq \sqrt{n} \varepsilon(2+\sqrt{n} \varepsilon)
\end{align}

\begin{equation}
\operatorname{cond}_{2}(A M) \leq \sqrt{\frac{1+\delta}{1-\delta}}
\end{equation}

Finally, applying Lemma 3 to the matrix $ A M M^{T} A^{T} $ instead of $ A M$ we conclude that the singular values are indeed clustered at 1 and lie inside the interval \([\sqrt{1-\delta}, \sqrt{1+\delta}]\).
\end{proof}

\begin{section}{MR algorithm}
	
	The minimal residual iteration (aka MR) also attempts to find an approximate sparse inverse via minimizing the Frobenius norm.
	
	As in SPAI, it solves each column of $M$ independently. For each column we run $n_{i}$ iterations on a descent method that minimizes that iteratively minimizes the residual while also performing numerical dropping to enforce sparsity in $M$.
	
	First initialise $M$, for example $M = \alpha Id$ or $M = A^{T}$.
	Then for $n_{i}$ iterations update $m{j}$ as follows ...
	
	\begin{equation}
	\begin{array}{l}{r_{j} :=e_{j}-A m_{j}} \\ {\alpha_{j} :=\frac{\left(r_{j}, A r_{j}\right)}{\left(A r_{j}, A r_{j}\right)}} \\ {m_{j} :=m_{j}+\alpha_{j} r_{j}} \\ {\text {Apply numerical dropping to } m_{j}}\end{array}
	\end{equation}
	see proof of step bellow
	
Numerical dropping ....

\begin{equation}
\rho_{i j}=-2 m_{i j}\left(e_{i}, A^{T} r_{j}\right)+m_{i j}^{2}
\end{equation}

keep the  lfil elements that have the largest $\left|\rho_{i j}\right|$

Proof here of step

Let $A$ be a matrix and $r, e, m$ be vectors s.t. $r = e - Am$. If we want to minimize the norm of $\hat{r}$; where $\hat{r} = e - A\hat{m}$, s.t. $\hat{m}$ lies along the path $m + \alpha r$. \\
Then, the optimal step size is $\alpha = \frac{\langle\,r,Ar\rangle}{\|Ar\|_{2}^{2}}$
\\\\
Proof 
\\\\
We compute the gradient of $\|\hat{r}\|_{2}$ w.r.t. $\alpha$ to find the optimum step size. 




\begin{equation}
\begin{aligned}
\frac{\partial \|\hat{r} \|_{2}^{2}}{\partial \alpha} 
& = \frac{\partial \|e - A\hat{m}\|_{2}^{2}}{\partial \alpha} \\
& = \frac{\partial \|e - A(m + \alpha r)\|_{2}^{2}}{\partial \alpha} \\
& = 2\alpha \|Ar\|_{2}^{2}  + 2(Am)^{T}Ar - 2e^{T}Ar \\
& = 2\alpha \|Ar\|_{2}^{2}  + 2(Am - e)^{T}Ar \\
& = 2\alpha \|Ar\|_{2}^{2}  - 2r^{T}Ar \\
\\
\end{aligned}
\end{equation}

By setting the derivative to $0$ we find that $\alpha = \frac{\langle\,r,Ar\rangle}{\|Ar\|_{2}^{2}}$


\end{section}


\newpage

\begin{section}{Experiments}
	
	\begin{table}[]
		\centering
	\begin{tabular}{@{}cccccc@{}}
\toprule
		$\varepsilon$ &    $||AM-I||_F$ &  $||AM-I||_2$ &   $||AM-I||_1$ &  $\operatorname{cond}_2(AM)$   & $\frac{{nz}(M)}{{nz}(a)}$ \\
		\midrule
		0.60 & 14.26 &  1.02  &  1.22 & 1989.98  &  0.32 \\

		0.50 & 11.29 &  1.08  &  1.56 & 210.79  &  0.61 \\

		0.40 &  8.97 &  1.05  &  1.56 & 74.15  &   0.89 \\

		0.30  & 7.12  & 1.00  &  1.76 & 35.13  &  1.53 \\

		0.20 &  4.81 &  0.98  &  1.72 &  12.82 &   3.41 \\
\bottomrule
	\end{tabular}
		\caption{}
	\label{tab:grote2}
\end{table}

	\begin{table}[]
		\centering
	\begin{tabular}{@{}cc@{}}
		\toprule
		$\varepsilon$ & Bi-CGSTAB \\
		\midrule
		0.6    &   172.5       \\     
		0.5    &   59.5        \\    
		0.4    &   33.5        \\    
		0.3    &   29.5        \\    
		0.2    &   15.5 \\
		\bottomrule
	\end{tabular}
	\caption{}
	\label{tab:grote1}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
	\centering
	\begin{tabular}{@{}cccc@{}}
		\toprule
		SPAI error threshold & BICGSTAB tolerance & \# Iterations & Relative residual  \\ \midrule
		\multirow{3}{*}{0.5}& 1e-08 & 9.0 & 1e-08 \\
			& 1e-12 & 93.0 & 4e-13 \\
			& 1e-16 & 156.5 & 2e-16 \\
		\midrule
		\multirow{3}{*}{0.3} & 1e-08	& 8.5	& 9e-09 \\
		& 1e-12	& 73.5	& 1e-12 \\
		& 1e-16	& 122.5	& 5e-16 \\
		\bottomrule
	\end{tabular}
	\caption{}
	\label{tab:benzi}
\end{table}

\end{section}
\clearpage
\section{Appendix}

\subsection{Orthonal invariance of the Frobenius norm}\label{sec:ortho}
Let us consider a matrix $R$ and on applying orthogonal matrices on both sides in the form of $QRQ^{T}$. We show that the Frobenius norm remains invariant to such an orthogonal transformation. \\

The following chain of equalities follows by using the orthogonality of
Q as well as the cyclical property of the trace.
\begin{equation}
\begin{aligned}
\|QRQ^{T} \|_{F}^{2}
& =  tr((QRQ^{T})^{T}QRQ^{T}) \\
& =  tr(QR^{T}Q^{T}QRQ^{T}) \\
& =  tr(QR^{T}RQ^{T}) \\
& =  tr(R^{T}RQ^{T}Q) \\
& =  tr(R^{T}R) \\
& =  \|R\|_{F}^{2} 
\end{aligned}
\end{equation}
	
\subsection{Some corollaries of the Cauchy Schwartz inequality} \label{sec:csi}

Cauchy Schwartz states that:

\begin{equation}
\langle\,x,y\rangle \leq \|x\|_{2}\|y\|_{2}
\end{equation}

By choosing $y$ judiciously we get the two following additional inequalities.
\\
Let $x = (x_{1}, ..., x_{n})$ and $y = (1, 1, ..., 1)$

\begin{equation}
\sum_{i=1}^{n} x_{i} = \langle\,x,y\rangle  \leq  \|x\|_{2}\|y\|_{2} = \sqrt{n}\|x\|_{2}
\end{equation}

We can improve the inequality by choosing $y_{i} = sign(x_{i})$. 

\begin{equation}
\sum_{i=1}^{n} \left|x_{i}\right| =  \|x\|_{1}\leq \sqrt{n}\|x\|_{2}
\end{equation}	

%TODO
% Make it lemma 1.1 and 1.2 and so on! 
\end{document}